{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBox 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0gr6KvsmAHN",
        "outputId": "b39c73b0-f64d-4100-eb69-3ba74608eceb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dhqqja26mwwn",
        "outputId": "3a8647dd-bab2-4d4b-c220-6506097311e0"
      },
      "source": [
        "!pip install mlbox==0.8.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlbox==0.8.5\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/26/6236ca21e762067fbb7a6cd388fc9812380af88ae007ca42da9ef6384ed8/mlbox-0.8.5.tar.gz\n",
            "Collecting numpy==1.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ce/d0b92f0283faa4da76ea82587ff9da70104e81f59ba14f76c87e4196254e/numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 203kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from mlbox==0.8.5) (1.4.1)\n",
            "Collecting matplotlib==3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/2a/e47bbd9396af32376863a426baed62d9bf3091f81defd1fe81c5f33b11a3/matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 149kB/s \n",
            "\u001b[?25hCollecting hyperopt==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/11/8bbbb5edb78c40a2bd0f6b730e3dc0f29ffbaea9a59520eb9622951e9151/hyperopt-0.2.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 47.0MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 26.3MB/s \n",
            "\u001b[?25hCollecting joblib==0.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 39.8MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/db/7d8204ddba84ab5d1e4fd1af8f82bbe39c589488bee71e45c662f4144010/scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 20.6MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 54kB/s \n",
            "\u001b[?25hCollecting lightgbm==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 19.3MB/s \n",
            "\u001b[?25hCollecting tables==3.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/5e/0463e9a2ae44838e36ca81dfe8a6e056b15cc15be77e7f8feae116f02ec7/tables-3.5.2-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 39.8MB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox==0.8.5) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox==0.8.5) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox==0.8.5) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox==0.8.5) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox==0.8.5) (4.41.1)\n",
            "Collecting networkx==2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox==0.8.5) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox==0.8.5) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox==0.8.5) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->mlbox==0.8.5) (2018.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 39.7MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (0.36.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox==0.8.5) (0.2.0)\n",
            "Collecting mock>=2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables==3.5.2->mlbox==0.8.5) (2.7.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.2->hyperopt==0.2.3->mlbox==0.8.5) (4.4.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0->mlbox==0.8.5) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0->mlbox==0.8.5) (56.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (1.28.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.10.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox==0.8.5) (0.4.8)\n",
            "Building wheels for collected packages: mlbox, networkx, gast\n",
            "  Building wheel for mlbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mlbox: filename=mlbox-0.8.5-cp37-none-any.whl size=43754 sha256=a7fff54b2133d6299daa3c8aa48c4241b04b6b2f0b2ab00a2e2ee6948c2b8e4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/71/0e/b72acfbcdaaf1e1480d9e7e50402b916093be47738a9d46ceb\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.2-py2.py3-none-any.whl size=1527322 sha256=1029c95aa16b4a3a810797a2dc664f95ce4bce6fd0a060c80bfbe06da5bd0b1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=da38b1c702fcafbe1bad6de6d94093de5780021d9aa9ce5b1c2932d4658c61c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built mlbox networkx gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, matplotlib, networkx, hyperopt, pandas, joblib, scikit-learn, tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow, lightgbm, mock, tables, xlrd, mlbox\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: networkx 2.5.1\n",
            "    Uninstalling networkx-2.5.1:\n",
            "      Successfully uninstalled networkx-2.5.1\n",
            "  Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed gast-0.2.2 hyperopt-0.2.3 joblib-0.14.1 keras-applications-1.0.8 lightgbm-2.3.1 matplotlib-3.0.3 mlbox-0.8.5 mock-4.0.3 networkx-2.2 numpy-1.18.2 pandas-0.25.3 scikit-learn-0.22.1 tables-3.5.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ROgC114ma07"
      },
      "source": [
        "# from mlbox.preprocessing import *\n",
        "# from mlbox.optimisation import *\n",
        "# from mlbox.prediction import *\n",
        "# import mlbox\n",
        "from mlbox.preprocessing import Reader\n",
        "from mlbox.preprocessing import Drift_thresholder\n",
        "from mlbox.optimisation import Optimiser\n",
        "from mlbox.prediction import Predictor\n",
        "# import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZQBPgZymvZF"
      },
      "source": [
        "#for Housing regression price\n",
        "# train_path = '/content/drive/MyDrive/dataset/kaggle/advanced_housing_regression/train.csv'\n",
        "# test_path = '/content/drive/MyDrive/dataset/kaggle/advanced_housing_regression/test.csv'\n",
        "# paths = [train_path,test_path ] #actual\n",
        "# target_name = \"SalePrice\"\n",
        "\n",
        "#for Credit Card\n",
        "# train_path = '/content/drive/MyDrive/dataset/kaggle/credit_card/UCI_Credit_Card_train.csv'\n",
        "# test_path = '/content/drive/MyDrive/dataset/kaggle/credit_card/UCI_Credit_Card_test.csv'\n",
        "# paths = [train_path,test_path ] #actual\n",
        "# target_name = \"default.payment.next.month\"\n",
        "\n",
        "\n",
        "train_path = '/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_train.csv'\n",
        "test_path = '/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_test.csv'\n",
        "paths = [train_path,test_path ] #actual\n",
        "target_name = \"y\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "qgEh4-Z4pZsD",
        "outputId": "0c944bc8-0527-4933-cd22-0c7fe16b8196"
      },
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv(train_path)\n",
        "df2 = pd.read_csv(test_path)\n",
        "\n",
        "df1.fillna(0, inplace = True)\n",
        "df1.to_csv('/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_train_fillnaed.csv')\n",
        "df1\n",
        "\n",
        "df2.fillna(0, inplace = True)\n",
        "df2.to_csv('/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_test_fillnaed.csv')\n",
        "\n",
        "train_path = '/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_train_fillnaed.csv'\n",
        "test_path = '/content/drive/MyDrive/dataset/Sample_Data_03/dataset_03_with_header_test_fillnaed.csv'\n",
        "\n",
        "df2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>x001</th>\n",
              "      <th>x002</th>\n",
              "      <th>x003</th>\n",
              "      <th>x004</th>\n",
              "      <th>x005</th>\n",
              "      <th>x006</th>\n",
              "      <th>x007</th>\n",
              "      <th>x008</th>\n",
              "      <th>x009</th>\n",
              "      <th>x010</th>\n",
              "      <th>x011</th>\n",
              "      <th>x012</th>\n",
              "      <th>x013</th>\n",
              "      <th>x014</th>\n",
              "      <th>x015</th>\n",
              "      <th>x016</th>\n",
              "      <th>x017</th>\n",
              "      <th>x018</th>\n",
              "      <th>x019</th>\n",
              "      <th>x020</th>\n",
              "      <th>x021</th>\n",
              "      <th>x022</th>\n",
              "      <th>x023</th>\n",
              "      <th>x024</th>\n",
              "      <th>x025</th>\n",
              "      <th>x026</th>\n",
              "      <th>x027</th>\n",
              "      <th>x028</th>\n",
              "      <th>x029</th>\n",
              "      <th>x030</th>\n",
              "      <th>x031</th>\n",
              "      <th>x032</th>\n",
              "      <th>x033</th>\n",
              "      <th>x034</th>\n",
              "      <th>x035</th>\n",
              "      <th>x036</th>\n",
              "      <th>x037</th>\n",
              "      <th>x038</th>\n",
              "      <th>x039</th>\n",
              "      <th>...</th>\n",
              "      <th>x266</th>\n",
              "      <th>x267</th>\n",
              "      <th>x268</th>\n",
              "      <th>x269</th>\n",
              "      <th>x270</th>\n",
              "      <th>x271</th>\n",
              "      <th>x272</th>\n",
              "      <th>x273</th>\n",
              "      <th>x274</th>\n",
              "      <th>x275</th>\n",
              "      <th>x276</th>\n",
              "      <th>x277</th>\n",
              "      <th>x278</th>\n",
              "      <th>x279</th>\n",
              "      <th>x280</th>\n",
              "      <th>x281</th>\n",
              "      <th>x282</th>\n",
              "      <th>x283</th>\n",
              "      <th>x284</th>\n",
              "      <th>x285</th>\n",
              "      <th>x286</th>\n",
              "      <th>x287</th>\n",
              "      <th>x288</th>\n",
              "      <th>x289</th>\n",
              "      <th>x290</th>\n",
              "      <th>x291</th>\n",
              "      <th>x292</th>\n",
              "      <th>x293</th>\n",
              "      <th>x294</th>\n",
              "      <th>x295</th>\n",
              "      <th>x296</th>\n",
              "      <th>x297</th>\n",
              "      <th>x298</th>\n",
              "      <th>x299</th>\n",
              "      <th>x300</th>\n",
              "      <th>x301</th>\n",
              "      <th>x302</th>\n",
              "      <th>x303</th>\n",
              "      <th>x304</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90000</td>\n",
              "      <td>883561</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>132</td>\n",
              "      <td>132</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90001</td>\n",
              "      <td>883297</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90002</td>\n",
              "      <td>1063945</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.9703</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6176</td>\n",
              "      <td>20901</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20901</td>\n",
              "      <td>0</td>\n",
              "      <td>20901</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>20901</td>\n",
              "      <td>20901</td>\n",
              "      <td>0.9703</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>20901</td>\n",
              "      <td>0.9703</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90003</td>\n",
              "      <td>1214266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90004</td>\n",
              "      <td>1091955</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.1067</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1067</td>\n",
              "      <td>1410</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1410</td>\n",
              "      <td>0</td>\n",
              "      <td>1410</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0000</td>\n",
              "      <td>1410</td>\n",
              "      <td>1410</td>\n",
              "      <td>0.1067</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1410</td>\n",
              "      <td>0.1067</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>99995</td>\n",
              "      <td>1573467</td>\n",
              "      <td>200.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.9166</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3308</td>\n",
              "      <td>37362</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30960</td>\n",
              "      <td>0</td>\n",
              "      <td>30960</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>157.4000</td>\n",
              "      <td>30960</td>\n",
              "      <td>30960</td>\n",
              "      <td>1.2621</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>30960</td>\n",
              "      <td>1.2621</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20277</td>\n",
              "      <td>1.5749</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>99996</td>\n",
              "      <td>1653422</td>\n",
              "      <td>292.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0872</td>\n",
              "      <td>36379</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36379</td>\n",
              "      <td>0</td>\n",
              "      <td>36379</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>80.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>31202</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>99997</td>\n",
              "      <td>1284669</td>\n",
              "      <td>35.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>30.0</td>\n",
              "      <td>31.5</td>\n",
              "      <td>0.3940</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.4824</td>\n",
              "      <td>31058</td>\n",
              "      <td>27971</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>20507</td>\n",
              "      <td>20237</td>\n",
              "      <td>270</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>27.3999</td>\n",
              "      <td>13100</td>\n",
              "      <td>13100</td>\n",
              "      <td>0.5592</td>\n",
              "      <td>13100</td>\n",
              "      <td>0.5592</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>99998</td>\n",
              "      <td>1434877</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.1650</td>\n",
              "      <td>699</td>\n",
              "      <td>378</td>\n",
              "      <td>1.2600</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>699</td>\n",
              "      <td>378</td>\n",
              "      <td>321</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>99999</td>\n",
              "      <td>1596945</td>\n",
              "      <td>134.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>678.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0707</td>\n",
              "      <td>18708</td>\n",
              "      <td>1511</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>18482</td>\n",
              "      <td>4999</td>\n",
              "      <td>13483</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>112.0000</td>\n",
              "      <td>16447</td>\n",
              "      <td>16447</td>\n",
              "      <td>1.3517</td>\n",
              "      <td>3714</td>\n",
              "      <td>1.1727</td>\n",
              "      <td>12733</td>\n",
              "      <td>1.4147</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>16447</td>\n",
              "      <td>1.3517</td>\n",
              "      <td>554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 306 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0     x001   x002  x003   x004  ...  x301  x302   x303    x304    y\n",
              "0          90000   883561    0.0   0.0    0.0  ...     1   5.0      0  0.0000  487\n",
              "1          90001   883297    0.0   0.0    0.0  ...     0   0.0      0  0.0000  505\n",
              "2          90002  1063945    4.0   4.0    4.0  ...     0   0.0      0  0.0000  725\n",
              "3          90003  1214266    0.0   0.0    0.0  ...     0   0.0      0  0.0000  501\n",
              "4          90004  1091955   50.0  50.0   50.0  ...     0   0.0      0  0.0000  587\n",
              "...          ...      ...    ...   ...    ...  ...   ...   ...    ...     ...  ...\n",
              "9995       99995  1573467  200.0   3.0  157.0  ...     1   5.0  20277  1.5749  578\n",
              "9996       99996  1653422  292.0  80.0  159.0  ...     0   0.0      0  0.0000  835\n",
              "9997       99997  1284669   35.0   4.0   26.0  ...     0   0.0      0  0.0000  425\n",
              "9998       99998  1434877    4.0   3.0    3.0  ...     0   0.0      0  0.0000  398\n",
              "9999       99999  1596945  134.0  19.0   75.0  ...     1   5.0  16447  1.3517  554\n",
              "\n",
              "[10000 rows x 306 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQIlKlamvXF",
        "outputId": "438d6ad0-64ef-4b64-adbb-db8df0f8a351"
      },
      "source": [
        "rd = Reader(sep=',')\n",
        "# Return a dictionnary containing three entries\n",
        "# dict[\"train\"] contains training samples withtout target columns\n",
        "# dict[\"test\"] contains testing elements withtout target columns\n",
        "# dict[\"target\"] contains target columns for training samples.\n",
        "data = rd.train_test_split(paths, target_name)\n",
        "\n",
        "dft = Drift_thresholder()\n",
        "data = dft.fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "reading csv : dataset_03_with_header_train.csv ...\n",
            "cleaning data ...\n",
            "CPU time: 20.406578063964844 seconds\n",
            "\n",
            "reading csv : dataset_03_with_header_test.csv ...\n",
            "cleaning data ...\n",
            "CPU time: 2.324070692062378 seconds\n",
            "\n",
            "You have no test dataset !\n",
            "\n",
            "> Number of common features : 304\n",
            "\n",
            "gathering and crunching for train and test datasets ...\n",
            "reindexing for train and test datasets ...\n",
            "dropping training duplicates ...\n",
            "dropping constant variables on training set ...\n",
            "\n",
            "> Number of categorical features: 0\n",
            "> Number of numerical features: 300\n",
            "> Number of training samples : 100000\n",
            "> Number of test samples : 0\n",
            "\n",
            "> Top sparse features (% missing values on train set):\n",
            "x242    93.3\n",
            "x295    86.5\n",
            "x304    81.9\n",
            "x098    80.7\n",
            "x155    79.1\n",
            "dtype: float64\n",
            "\n",
            "> Task : regression\n",
            "count    100000.000000\n",
            "mean        619.198230\n",
            "std         118.461932\n",
            "min         300.000000\n",
            "25%         524.000000\n",
            "50%         599.000000\n",
            "75%         720.000000\n",
            "max         839.000000\n",
            "Name: y, dtype: float64\n",
            "\n",
            "You have no test dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13DOhcvcmvVP",
        "outputId": "d9045de0-9f85-4087-ff61-91d89b724ffb"
      },
      "source": [
        "opt = Optimiser(scoring='accuracy', n_folds=3)\n",
        "opt.evaluate(None, data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:74: UserWarning: Optimiser will save all your fitted models into directory 'save/joblib'. Please clear it regularly.\n",
            "  +str(self.to_path)+\"/joblib'. Please clear it regularly.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No parameters set. Default configuration is tested\n",
            "\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            "\n",
            ">>> NA ENCODER :{'numerical_strategy': 'mean', 'categorical_strategy': '<NULL>'}\n",
            "\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            "\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "\n",
            "\n",
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 36.76301574707031 seconds\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0xdi9S3mvTc",
        "outputId": "231a5a89-b471-4222-fe3b-410f6bd134ec"
      },
      "source": [
        "space = {'ne__numerical_strategy': {\"search\": \"choice\", \"space\": [0]},\n",
        "         'ce__strategy': {\"search\": \"choice\",\n",
        "                          \"space\": [\"label_encoding\",\n",
        "                                    \"random_projection\",\n",
        "                                    \"entity_embedding\"]},\n",
        "         'fs__threshold': {\"search\": \"uniform\",\n",
        "                           \"space\": [0.01, 0.3]},\n",
        "         'est__max_depth': {\"search\": \"choice\",\n",
        "                            \"space\": [3, 4, 5, 6, 7]}\n",
        "\n",
        "         }\n",
        "\n",
        "# Optimises hyper-parameters of the whole Pipeline with a given scoring\n",
        "# function. Algorithm used to optimize : Tree Parzen Estimator.\n",
        "#\n",
        "# IMPORTANT : Try to avoid dependent parameters and to set one feature\n",
        "# selection strategy and one estimator strategy at a time.\n",
        "best = opt.optimise(space, data, 15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.2486578615650148}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 4, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 48.2007577419281 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.19553223300825756}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 4, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "  7%|▋         | 1/15 [00:48<11:14, 48.21s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 48.07478928565979 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.2775228999221713}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 7, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 13%|█▎        | 2/15 [01:36<10:26, 48.17s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 59.847856283187866 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.03926021403984078}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 3, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 20%|██        | 3/15 [02:36<10:20, 51.68s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 43.391637325286865 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.14261061478504478}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 7, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 27%|██▋       | 4/15 [03:19<09:01, 49.20s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 60.936848402023315 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.15865076896803448}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 3, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 33%|███▎      | 5/15 [04:20<08:47, 52.73s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 43.53529715538025 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.2039163734752001}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 5, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 40%|████      | 6/15 [05:04<07:29, 49.98s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 53.72337579727173 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.18764620788079056}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 7, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 47%|████▋     | 7/15 [05:57<06:48, 51.10s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 59.53952097892761 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.18352349679429988}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 3, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 53%|█████▎    | 8/15 [06:57<06:15, 53.64s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 44.30951189994812 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.21246879102901717}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 4, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 60%|██████    | 9/15 [07:41<05:05, 50.84s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 48.30839991569519 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'entity_embedding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.17027650902777536}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 7, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 67%|██████▋   | 10/15 [08:30<04:10, 50.09s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.77s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.78s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.78s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 68.0978684425354 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'entity_embedding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.211787057045121}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 3, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 73%|███████▎  | 11/15 [09:38<03:41, 55.49s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.78s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 47.9622688293457 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'entity_embedding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.27124356986784964}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 80%|████████  | 12/15 [10:26<02:39, 53.24s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.76s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 62.17027807235718 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.09660480318915134}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 87%|████████▋ | 13/15 [11:28<01:51, 55.92s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 57.73117756843567 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'l1', 'threshold': 0.015188930989567818}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample': 0.9, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 93%|█████████▎| 14/15 [12:26<00:56, 56.47s/trial, best loss: inf]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164909147.59404975, tolerance: 93287.1368339784\n",
            "  positive)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = -inf\n",
            "VARIANCE : nan (fold 1 = -inf, fold 2 = -inf, fold 3 = -inf)\n",
            "CPU time: 57.67656087875366 seconds\n",
            "100%|██████████| 15/15 [13:23<00:00, 53.58s/trial, best loss: inf]\n",
            "\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BEST HYPER-PARAMETERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "{'ce__strategy': 'random_projection', 'est__max_depth': 4, 'fs__threshold': 0.2486578615650148, 'ne__numerical_strategy': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:441: UserWarning: An error occurred while computing the cross validation mean score. Please check that the parameter values are correct and that your scoring function is valid and appropriate to the task.\n",
            "  warnings.warn(\"An error occurred while computing the cross \"\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "1SCja672mvRx",
        "outputId": "ff057091-91b7-4fde-af0a-ee32fd9c9118"
      },
      "source": [
        "prd = Predictor()\n",
        "prd.fit_predict(best, data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "fitting the pipeline ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 220885137.6436383, tolerance: 140330.88954867117\n",
            "  positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU time: 78.33524918556213 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Feature importances dumped into directory : save\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/prediction/predictor.py:405: UserWarning: You have no test dataset. Cannot predict !\n",
            "  warnings.warn(\"You have no test dataset. Cannot predict !\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<mlbox.prediction.predictor.Predictor at 0x7f6a1b434410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8cGiPhksc-y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}